{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b674364-b176-4812-9411-ff52ce071cea",
   "metadata": {},
   "source": [
    "# Spooky Author Identification\n",
    "\n",
    "You may (or may not) have heard of the below 3 authors:\n",
    "\n",
    "- **Mary Wollstonecraft Shelley** was an English novelist famous for writing \"Frankenstein\" in 1818.\n",
    "- **Edgar Allen Poe** was an American writer and poet famous for his writing including \"The Raven\" (1845) and \"The Tell-Tale Heart\" (1843)\n",
    "- **H.P. Lovecraft** was an American horror fiction writer who is known for writing \"The Call of Cthulhu\" (1928)\n",
    "\n",
    "... also all three have been referenced in \"The Simpsons\" which some of us may consider to be more noteworthy.\n",
    "\n",
    "We will generally call these 3 writers \"Spooky\" Authors. For those who are really familiar with the above authors they may be able to read a short passage of writing and determine which of the 3 authors wrote the piece. Our task today will be to write a computer program that will do just that; given a short passage of writing will correctly identify its author.\n",
    "\n",
    "*The data and inspiration for this work come from: https://www.kaggle.com/c/spooky-author-identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738f167e-6a11-44ef-8f11-5f0d883d9f29",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "Our approach is fairly simple (and perhaps overly naive). We will examine a bunch of quotes from each of the above authors and we will use this data to determine how frequently they use each word in their writing. \n",
    "\n",
    "Then for an unclassified quote, for each possible author, we will compute the product of frequencies for the words in that quote. The author having the highest product is guessed to be the author of the quote.\n",
    "\n",
    "Lastly we will compare our guess against the actual author and track our success rate.\n",
    "\n",
    "Consider a simple example:\n",
    "\n",
    "- Author 1 Quotes: `[ \"banana banana orange\", \"orange black yellow\", \"banana yellow\"]`\n",
    "\n",
    "- Author 2 Quotes: `[ \"gold banana\", \"texas tea\", \"gold tea\", \"orange pekoe\"]`\n",
    "\n",
    "Author 1 frequencies: \"banana\": `3/8`, \"orange\": `2/8`, \"black\": `1/8`, \"yellow\": `2/8`\n",
    "\n",
    "\n",
    "Author 2 frequencies: \"gold\": `2/8`, \"banana\": `1/8`, \"texas\": `1/8`, \"tea\": `2/8`, \"orange\": `1/8`, \"pekoe\": `1/8`\n",
    "\n",
    "Now for the quote: *\"orange banana\"*\n",
    "\n",
    "We consider that Author 1 has product: `2/8 (orange) * 3/8 (banana) = 6/64` \n",
    "\n",
    "Author 2 has product: `1/8 (orange) * 1/8 (banana) = 1/64`\n",
    "\n",
    "Therefore we guess that **Author 1** is the Author of the quote since `6/64 > 1/64`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85492f5e-7c0e-4fcc-a376-ba1a468da728",
   "metadata": {},
   "source": [
    "## Groups\n",
    "\n",
    "Today's case study will be completed in groups of 3 - 5 students each.\n",
    "\n",
    "Please assemble into groups of 3-5 by asking your neighbours to join the group.\n",
    "\n",
    "# Task 1: Introduction (3 minutes)\n",
    "\n",
    "1. Introduce yourselves\n",
    "2. Appoint one person as the **paper recorder** - this person should have a pen and a paper\n",
    "3. Appoint one person as the **computer recorder** - this person should have a connected device (laptop preferable)\n",
    "4. Appoint one person as the **communicator** - this person will speak to the instructor, or class and/or write stuff on the board if required\n",
    "5. The computer recorder should open this notebook and then proceed to download the data (next 2 cells below)\n",
    "\n",
    "   a. make sure code ai suggestions is turned off so we can all practice our coding with dictionaries\n",
    "6. Save the notebook to your local google drive \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a94ee36-f467-4a36-80da-639a6deb3b80",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "As is common in machine learning frameworks, we will be given a **training** data set and a **testing** data set.\n",
    "\n",
    "Each dataset consists of a single .csv file.\n",
    "\n",
    "A CSV file is a common-separated-values format where each line in the file is a row in a table, where each column is separated by commas. In our case each line will contain 3 columns of data, an id, a quote, and the author.\n",
    "\n",
    "Example:\n",
    "\n",
    "`\"id10633\",\"It was the beating of the old man's heart.\",\"EAP\"`\n",
    "\n",
    "In the above there is an id: `\"id10633\"`, a quote: `\"It was the beating of hte old man's heart.\"`, and the author: `\"EAP\"` denoting Edgar Allen Poe.\n",
    "\n",
    "Each author will be denoted by initials: `EAP` for Edgar Allen Poe, `MWS` for Mary Wollstonecraft Shelley and `HPL` for H.P. Lovecraft.\n",
    "\n",
    "We will use the `train.csv` dataset to build up a model for the frequencies of words each of our 3 authors use. Then we will see how this simple model can identify each model when we show it the test.csv dataset.\n",
    "\n",
    "* Note: mostly we will be practicing some of our programming skills like dictionary use rather than building a quality NLP (natural language processing) model to accomplish this task. But you can always explore the actual approaches people used at the above kaggle link. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba152fde-46e2-40b1-80bd-82840c3a4440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this case study to your own drive\n",
    "# the following code will download the data files required for this case study\n",
    "\n",
    "from os.path import basename, exists\n",
    "\n",
    "def download(url):\n",
    "    filename = basename(url)\n",
    "    if not exists(filename):\n",
    "        from urllib.request import urlretrieve\n",
    "\n",
    "        local, _ = urlretrieve(url, filename)\n",
    "        print(\"Downloaded \" + str(local))\n",
    "    return filename\n",
    "\n",
    "examples_path = \"https://github.com/andrewgodbout/F2025CS1910/raw/refs/heads/main/CaseStudies/data.zip\"\n",
    "\n",
    "\n",
    "download(examples_path)\n",
    "!unzip -o data.zip\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b16950-dc80-4d51-b295-03e28ded25a0",
   "metadata": {},
   "source": [
    "## csv Module\n",
    "\n",
    "From last week we learned about reading from files and spliting based on delimiters (like commas ,) but ...\n",
    "\n",
    "Rather than writing our own csv parsing code we will use the csv module. This module has a `DictReader` class that is capable of \n",
    "converting each row of the csv file into its own dictionary. In our case it will have `id`, `quote` and `author` keys:\n",
    "\n",
    "example of a quote attributed to Edgar Allan Poe (EAP):\n",
    "```python\n",
    "{ 'id' : '001',\n",
    "  'quote' : 'nevermore',\n",
    "  'author': 'EAP'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b87495-cf43-4f00-a27d-c0d20029b019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the csv module to load the data\n",
    "import csv\n",
    "with open('train.csv') as csvfile:\n",
    "\n",
    "    #DictReader will populate a dictionary for each line in the csv\n",
    "    reader = csv.DictReader(csvfile, fieldnames=[\"id\", \"quote\", \"author\"])\n",
    "\n",
    "    #create a counter so we can just print the first two records in the file\n",
    "    cnt = 0\n",
    "    #each row is a dictionary\n",
    "    for row in reader:\n",
    "        if cnt < 2:\n",
    "            print(row)\n",
    "        cnt += 1\n",
    "    \n",
    "    print(\"\\n\\ntotal lines read:\", reader.line_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fd879c-5d46-46dc-b2cb-5e87a19b81da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fec8c8e5-a4bb-49a6-a549-5231a5032bfa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Group Task 2: Understanding the data (5 minutes)\n",
    "\n",
    "*The group paper recorder should put a paper in the middle of the work space so all can see*\n",
    "\n",
    "In your groups discuss the above code and below questions and record your answers on paper.\n",
    "\n",
    "1. **How many different authors are their in our dataset?**\n",
    "\n",
    "2. **The training dataset has 19479 records each made up of 3 fields: `id`, `quote` and `author`, which of the fields is least important to our goal?**\n",
    "\n",
    "In the above code the variable `row` is initialized as a dictionary with keys: `id`, `quote` and `author` for each record/line in the `train.csv` data file. Recall we want to know how frequently words are used by each author -> we want to count how often they use each word.\n",
    "\n",
    "3. **What do we want to do with each of these rows?**\n",
    "\n",
    "i.e., in plain english what do we want to do to the `row` variable on each iteration of the loop?\n",
    "\n",
    "Things to consider:\n",
    "\n",
    "**1. How do we access the data inside `row`?**\n",
    "\n",
    "\n",
    "**2. How do we want our data organized?**\n",
    "\n",
    "We want the count of words for each author\n",
    "\n",
    "Perhaps a dictionary for each author where the key is the word and the value is the count for that word?\n",
    "```python\n",
    "#edgar allan poe wrote about ravens and beating hearts:\n",
    "poe_dict = { 'raven' : 29, 'heart' : 22, ...}\n",
    "```\n",
    "\n",
    "**3. How should we process each row?**\n",
    "\n",
    "\n",
    "**4. Do we need to do any data pre-processing?**\n",
    "\n",
    "Does punctuation impact things? \n",
    "\n",
    "Do want to bother with words like \"the\"?\n",
    "\n",
    "Does Capitalization impact things?\n",
    "\n",
    "**5. Other considerations**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edd9b63-7e06-4be5-a88f-46bf86517c79",
   "metadata": {},
   "source": [
    "# Task 3 Pseudo Code on Paper (5 minutes) \n",
    "\n",
    "Write out a high level pseudo-code solution on paper for how to process the training data. High level means avoid the details. We are looking for less than 6 high level steps for how to process our train.csv file, i.e., how to determine the frequency of word use for each author.\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b12067-b029-4cfe-affa-37f11804b5ca",
   "metadata": {},
   "source": [
    "## Pre-processing \n",
    "\n",
    "We will clean the quote by pre-precessing it \n",
    "\n",
    "1. To avoid issues with punctuation, i.e., \"hello,\" being a different word from \"hello\" we will remove all punctuation from the quotes. \n",
    "2. To avoid issues with capitalization, i.e., \"hello\" being different from \"Hello\" we will convert all quotes to lower case\n",
    "3. To avoid issues with common words, i.e., analyzing \"and\", \"I\" or \"a\" we will remove these `STOP` words from the quote\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299cffe9-f643-4c5e-909f-f80102a709dd",
   "metadata": {},
   "source": [
    "## Task 4: Implement the below clean_quote method (5 minutes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b32190d-1eea-497e-9b1a-521e0b10c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import string\n",
    "\n",
    "#Task 4-1: Ask generative ai to suggest some stop words in a python list (including those from 1800 and 1900s writing. \n",
    "#          Populate them into the below list\n",
    "\n",
    "stop_words = [\n",
    "    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\",\n",
    "    \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\",\n",
    "    \"could\",\n",
    "    \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\",\n",
    "    \"each\",\n",
    "    \"few\", \"for\", \"from\", \"further\",\n",
    "    \"had\", \"has\", \"have\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\",\n",
    "    \"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\",\n",
    "    \"let's\",\n",
    "    \"me\", \"more\", \"most\", \"my\", \"myself\",\n",
    "    \"nor\", \"not\",\n",
    "    \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\n",
    "    \"same\", \"she\", \"should\", \"so\", \"some\", \"such\",\n",
    "    \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\",\n",
    "    \"under\", \"until\", \"up\",\n",
    "    \"very\",\n",
    "    \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"with\", \"would\",\n",
    "    \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
    "\"thou\", \"thee\", \"thy\", \"thine\", \"ye\", \"hath\", \"doth\", \"dost\", \"art\", \"shalt\", \"wilt\", \"hast\", \"wert\",\n",
    "    \"whilst\", \"ere\", \"oft\", \"nay\", \"yea\", \"saith\", \"unto\", \"wherefore\", \"whence\", \"whither\"\n",
    "]\n",
    "\n",
    "print(string.punctuation)\n",
    "\n",
    "def clean_quote(quote :str, stop_words: list[str]) -> list[str]:\n",
    "    \"\"\" For a given quote clean the quote in 3 ways\n",
    "    1. remove punctuation\n",
    "    2. convert to lower case\n",
    "    3. remove stop words \n",
    "\n",
    "    @param: quote (string) the given quote to clean\n",
    "    @param: stop_words (list of strings) a list of all of the words that should be removed from quote\n",
    "\n",
    "    @return a list of strings representing the words in the cleaned quote\n",
    "    \"\"\"\n",
    "\n",
    "    #step 1 convert the quote to lower case\n",
    "    #step 2: remove punctuation (keep only characters that aren't punctuation)\n",
    "    #        note: that the string module has a list of common punctuation \n",
    "    #              in a variable: string.punctuation\n",
    "\n",
    "    #step 1 and 2 are given for you:\n",
    "    clean = ''.join([char.lower() for char in quote if char not in string.punctuation])\n",
    "    \n",
    "\n",
    "    #step 3: keep only the non-stop words\n",
    "    #        note: if you split a string by \" \" (space) \n",
    "    #              you get a list of the \"words\" in that string you can iterate over\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #step 4: return a list of words that have been cleaned\n",
    "\n",
    "    #remove this placeholder code\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a9507c-7553-438a-a613-bf32d7654a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#once you have completed the above clean_quote the following code should produce no errors\n",
    "#don't forget to \"run\" the above cell before running this cell\n",
    "\n",
    "good_words = clean_quote(\"You say, \\\"Goodbye\\\" and I say, \\\"Hello, hello, HELLO\\\"!\", stop_words)\n",
    "assert \"HELLO\" not in good_words, \"capital HELLO test failed\"\n",
    "assert \"i\" not in good_words, \"stop word test failed\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcf2991-78c6-41ee-92f3-12091d041a0d",
   "metadata": {},
   "source": [
    "## Task 5: Implement Count Words and Convert to Frequency methods ( 5 minutes )\n",
    "\n",
    "`count_words` is only a couple of lines just be careful when dealing with words that might not yet be in the dictionary\n",
    "\n",
    "`convert_to_freqs` does not take as input the total number of words, how can we obtain this easily from input_dict?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1939f4f2-fef2-484b-bf93-875ef7a07e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(good_words :str, author_dict: dict[str, int]) -> None:\n",
    "    \"\"\" Given a list of words and a dictionary of word counts\n",
    "        increment the count for each word in the list\n",
    "\n",
    "        @param good_words a list of cleaned words\n",
    "        @param author_dict a dictionary with keys as strings (words) and \n",
    "                           value int representing the count of occurrences of that word\n",
    "        @return None nothing is returned\n",
    "    \"\"\"\n",
    "    \n",
    "    #hint use the .get method to avoid issues when incrementing a word for the first time\n",
    "    \n",
    "\n",
    "def convert_to_freqs(input_dict: dict[str, int]) -> dict[str, float]:\n",
    "    \"\"\" For a given input dictionary of words (str) and their respective counts (int)\n",
    "       return a new dictionary with the words (str) and their frequency of occurrence (float)\n",
    "\n",
    "       the frequency of a word is the count of that word divided by the total number of words (sum of the values).\n",
    "    \n",
    "        @param input_dict a dictionary of words and respective counts\n",
    "        @return dictionary with the same keys as input_dict but having values as the \n",
    "                frequency of occurrence of those words\n",
    "    \"\"\"\n",
    "\n",
    "    #remove this placeholder code\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52255f9f-d378-465d-a078-95cecb4a2836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when you complete your methods this code should run properly\n",
    "\n",
    "author_dict = {}\n",
    "good_words = [\"say\", \"goodbye\", \"say\", \"hello\", \"hello\", \"hello\"]\n",
    "count_words(good_words, author_dict)\n",
    "assert \"hello\" in author_dict and author_dict[\"hello\"] == 3, \"hello test failed expecting count=3\"\n",
    "assert \"i\" not in author_dict, \"stop word test failed, expecting \\\"i\\\" is a stop word\"\n",
    "assert len(author_dict) == len([\"say\", \"goodbye\", \"hello\"]), \"number of keys should be 3 test failed\"\n",
    "\n",
    "freq_dict = convert_to_freqs(author_dict)\n",
    "\n",
    "assert len(freq_dict) == len(author_dict), \"len should match output and input\"\n",
    "assert abs(freq_dict.get(\"goodbye\", 0) - author_dict.get(\"goodbye\", 0.1)/sum(author_dict.values())) < 0.00001, \"check freq calculation\"\n",
    "\n",
    "print(\"all tests complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdb0bd7-d773-4524-a8d4-7695eca51139",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Glue code \n",
    "The following code will process our training dataset by iterating over the records and calling our above methods\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#create 3 dictionaries to store the count of each word for each author\n",
    "eap = {}\n",
    "mws = {}\n",
    "hpl = {}\n",
    "\n",
    "\n",
    "with open('train.csv') as csvfile:\n",
    "\n",
    "    #DictReader will populate a dictionary for each line in the csv\n",
    "    reader = csv.DictReader(csvfile, fieldnames=[\"id\", \"quote\", \"author\"])\n",
    "\n",
    "    #create a counter so we can just print the first two records in the file\n",
    "    cnt = 0\n",
    "    for row in reader:\n",
    "\n",
    "        #task 4: clean the input and give back a list of cleaned words\n",
    "        good_words = clean_quote(row['quote'], stop_words)\n",
    "        \n",
    "        if cnt < 2:\n",
    "            print(\"original:\", row['quote'])\n",
    "            print (\"cleaned quote:\", good_words)\n",
    "        cnt += 1\n",
    "\n",
    "        author = row['author']\n",
    "\n",
    "        #set the correct dictionary\n",
    "        author_dict = eap\n",
    "        if author == \"MWS\":\n",
    "            author_dict = mws\n",
    "\n",
    "        elif author == \"HPL\":\n",
    "            author_dict = hpl\n",
    "\n",
    "        #Task 5 count of the cleaned words in an author_dictionary\n",
    "        count_words(good_words, author_dict)\n",
    "        \n",
    "    print(\"\\n\\ntotal lines read:\", reader.line_num)\n",
    "\n",
    "\n",
    "#task 5: convert the dictionary of counts into a dictionary of frequencies\n",
    "eap_f = convert_to_freqs(eap)\n",
    "mws_f = convert_to_freqs(mws)\n",
    "hpl_f = convert_to_freqs(hpl)\n",
    "\n",
    "print(\"total EAP words:\", len(eap))\n",
    "print(\"total MWS words:\",len(mws))\n",
    "print(\"total HPL words:\",len(hpl))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ee9609-14e4-4453-a0f1-0b5eff449355",
   "metadata": {},
   "source": [
    "## Task 6 Discussion and Pseudo-code (5 minutes)\n",
    "\n",
    "We have processed the training dataset and have 3 dictionaries giving the frequency of use of an authors words. Our algorithm is to compare an unseen quote to our model (3 dictionaries of author word frequencies) and then predict the author of the unseen quote by computing the product of frequencies for the words in the unseen quote. \n",
    "\n",
    "Our product of frequencies may end up multiplying many small numbers together, which can be problematic, to avoid this we will only multiply together the *X* words having the largest frequency for that author.\n",
    "\n",
    "Discuss how to implement this with your group and form pseudo code on paper for your solution\n",
    "\n",
    "Discussion points:\n",
    "\n",
    "1. Can we reuse any of our work from prior tasks?\n",
    "\n",
    "2. What happens if the frequency for a word is zero? What happens if a word shows up twice in the same quote?\n",
    "\n",
    "3. How can we limited our product calculation to include only the *k* (depth) largest values?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc495ed-58d8-49b2-899d-4ef86575b5e6",
   "metadata": {},
   "source": [
    "## Task 7 Implement the product_freq method (7 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66300532-598a-46d2-8247-7b748bf58f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to frequencies\n",
    "\n",
    "def product_freq(clean_quote:list[str], author_freq_dict:str, depth:int) -> float:\n",
    "    \"\"\"\n",
    "    For a given cleaned quote (list of strings) and an author_freq_dict of words and their \n",
    "    frequency of use, compute the product of frequencies for the words having the depth (int)\n",
    "    largest frequencies\n",
    "\n",
    "    In the case that a word in clean_quote is not present in the author_freq_dict then small_number should be its frequency.\n",
    "\n",
    "    @param clean_quote: a list of words \n",
    "    @param author_freq_dict: A dictionary of words and their associated frequency of use for a particular author\n",
    "    @param depth: The number of words to use in the product calculation (only the depth-most words), if depth is larger than \n",
    "                  the length of the quote then only len(clean_quote) words are used in the product. Depth is always larger than 0\n",
    "\n",
    "                  \n",
    "    @return a floating point number giving the product of word frequencies for the maximum depth largest frequencies.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    small_number = 0.0000001\n",
    "\n",
    "    #remove this placeholder code\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df897f11-6f0c-4cc7-a219-b413df514f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#once the above is complete the following tests should pass\n",
    "clean_quote = [\"worst\", \"episode\", \"ever\"]\n",
    "author_freq_dict = { \"lisa\": 0.1, \"needs\": 0.2, \"braces\":0.2, \"episode\": 0.35, \"ever\":0.15}\n",
    "\n",
    "threshold = 0.000001\n",
    "\n",
    "#depth longer than quote\n",
    "prod = product_freq(clean_quote, author_freq_dict, 4)\n",
    "assert abs(prod - .15 * 0.35) < threshold, f\"product frequency expecting {.15*.35} got {prod}\"\n",
    "\n",
    "#depth longer than author_freq_dict\n",
    "prod = product_freq(clean_quote, author_freq_dict, 3)\n",
    "assert abs(prod - .15 * 0.35) < threshold, f\"product frequency expecting {.15*.35} got {prod}\"\n",
    "\n",
    "#depth equals clean length\n",
    "prod = product_freq(clean_quote, author_freq_dict, 2)\n",
    "assert abs(prod - .15 * 0.35) < threshold, f\"product frequency expecting {.15*.35} got {prod}\"\n",
    "\n",
    "#depth shorter than quote\n",
    "prod = product_freq(clean_quote, author_freq_dict, 1)\n",
    "assert abs(prod - 0.35) < threshold, f\"product frequency expecting .35 got {prod}\"\n",
    "\n",
    "print(\"above test results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844cd6ce-1bc5-4fa0-9fad-a6f032c26b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the testing\n",
    "\n",
    "with open('test.csv') as csvfile:\n",
    "\n",
    "    #DictReader will populate a dictionary for each line in the csv\n",
    "    reader = csv.DictReader(csvfile, fieldnames=[\"id\", \"quote\", \"author\"])\n",
    "\n",
    "    #create a counter so we can just print the first two records in the file\n",
    "    cnt = 0\n",
    "    correct_cnt = 0\n",
    "    #experiment by changing the depth\n",
    "    depth = 10\n",
    "    for row in reader:\n",
    "\n",
    "        cnt += 1\n",
    "        author = row['author']\n",
    "\n",
    "        #reuse task 4\n",
    "        good_words = clean_quote(row['quote'], stop_words)        \n",
    "\n",
    "        #task 7 for each author\n",
    "        prod_eap = (product_freq(good_words, eap_f, depth), \"EAP\")\n",
    "        prod_mws = (product_freq(good_words, mws_f, depth), \"MWS\")\n",
    "        prod_hpl = (product_freq(good_words, hpl_f, depth), \"HPL\")\n",
    "\n",
    "        #the largest product is the winner (our guess of the author)\n",
    "        winner = max(prod_eap, prod_mws, prod_hpl)\n",
    "        print(f\"{author}{good_words[0:3]}  {winner}\")\n",
    "\n",
    "        if winner[1] == row['author']:\n",
    "            correct_cnt += 1\n",
    "            \n",
    "    print(correct_cnt)\n",
    "    print(cnt)\n",
    "    print(\"accuracy:\", correct_cnt/cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f83144-6167-4c2c-b684-2d197054c0fc",
   "metadata": {},
   "source": [
    "## Class Checkin\n",
    "What accuracy level did everyone achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03ffa26-94f8-4261-b76a-81938d8a2d36",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "\n",
    "What happens in the above if depth is changed?\n",
    "\n",
    "*depth = 1, 10, 25, or 100, for example*\n",
    "\n",
    "What is the role of `small_number`? \n",
    "\n",
    "*Why not just multiply by zero when we encounter words that are not in our training set?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59d8672-e5b8-4bdd-8389-b490fb832851",
   "metadata": {},
   "source": [
    "# Further reading\n",
    "\n",
    "If you found this interesting, natural language processing is an area of computer science / data science. Our approach is quite simple but not unrelated to cosine-similarity for comparing the \"closeness\" of things or TF-IDF (term frequency inverse document frequency) which is a way to represent word distributions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ad397f-5d27-42c8-8a74-f2adab9b287c",
   "metadata": {},
   "source": [
    "# share\n",
    "\n",
    "Share your group's version of the case study with your teammates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2cda62-4e38-4db6-9174-16f0b946304d",
   "metadata": {},
   "source": [
    "# on your own \n",
    "\n",
    "Can you modify the above code to determine accuracy for each individual author? \n",
    "\n",
    "Which author had the highest accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d500427-2f03-4274-a5a8-2776fafd4ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
