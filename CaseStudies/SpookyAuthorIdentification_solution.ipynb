{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b674364-b176-4812-9411-ff52ce071cea",
   "metadata": {},
   "source": [
    "# Spooky Author Identification\n",
    "\n",
    "You may (or may not) have heard of the below 3 authors:\n",
    "\n",
    "- **Mary Wollstonecraft Shelley** was an English novelist famous for writing \"Frankenstein\" in 1818.\n",
    "- **Edgar Allen Poe** was an American writer and poet famous for his writing including \"The Raven\" (1845) and \"The Tell-Tale Heart\" (1843)\n",
    "- **H.P. Lovecraft** was an American horror fiction writer who is known for writing \"The Call of Cthulhu\" (1928)\n",
    "\n",
    "... also all three have been referenced in \"The Simpsons\" which is also noteworthy.\n",
    "\n",
    "We will generally call these 3 writers \"Spooky\" Authors. For those who are really familiar with the above authors they may be able to read a short passage of writing and determine which of the 3 authors wrote the piece. Our task today will be to write a computer program that will do just that; given a short passage of writing will correctly identify its author.\n",
    "\n",
    "*The data and inspiration for this work come from: https://www.kaggle.com/c/spooky-author-identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738f167e-6a11-44ef-8f11-5f0d883d9f29",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "Our approach is fairly simple (and perhaps overly naive). We will examine a bunch of quotes from each of the above authors and we will use this data to determine how frequently they use each word in their writing. \n",
    "\n",
    "Then for an unclassified quote, for each possible author, we will compute the product of frequencies for the words in that quote. The author having the highest product is guessed to be the author of the quote.\n",
    "\n",
    "Lastly we will compare our guess against the actual author and track our success rate.\n",
    "\n",
    "Consider a simple example:\n",
    "\n",
    "- Author 1 Quotes: `[ \"banana banana orange\", \"orange black yellow\", \"banana yellow\"]`\n",
    "\n",
    "- Author 2 Quotes: `[ \"gold banana\", \"texas tea\", \"gold tea\", \"orange pekoe\"]`\n",
    "\n",
    "Author 1 frequencies: \"banana\": `3/8`, \"orange\": `2/8`, \"black\": `1/8`, \"yellow\": `2/8`\n",
    "\n",
    "\n",
    "Author 2 frequencies: \"gold\": `2/8`, \"banana\": `1/8`, \"texas\": `1/8`, \"tea\": `2/8`, \"orange\": `1/8`, \"pekoe\": `1/8`\n",
    "\n",
    "Now for the quote: *\"orange banana\"*\n",
    "\n",
    "We consider that Author 1 has product: `2/8 (orange) * 3/8 (banana) = 6/64` \n",
    "\n",
    "Author 2 has product: `1/8 (orange) * 1/8 (banana) = 1/64`\n",
    "\n",
    "Therefore we guess that **Author 1** is the Author of the quote since `6/64 > 1/64`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85492f5e-7c0e-4fcc-a376-ba1a468da728",
   "metadata": {},
   "source": [
    "## Groups\n",
    "\n",
    "Today's case study will be completed in groups of 3 - 5 students each.\n",
    "\n",
    "Please assemble into groups of 3-5 by asking your neighbours to join the group.\n",
    "\n",
    "# Task 1: Introduction (3 minutes)\n",
    "\n",
    "1. Introduce yourselves\n",
    "2. Appoint one person as the **paper recorder** - this person should have a pen and a paper\n",
    "3. Appoint one person as the **computer recorder** - this person should have a connected device (laptop preferable)\n",
    "4. Appoint one person as the **communicator** - this person will speak to the instructor, or class and/or write stuff on the board if required\n",
    "5. The computer recorder should open this notebook and then proceed to download the data (next 2 cells below)\n",
    "\n",
    "   a. make sure code ai suggestions is turned off so we can all practice our coding with dictionaries\n",
    "6. Save the notebook to your local google drive \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a94ee36-f467-4a36-80da-639a6deb3b80",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "As is common in machine learning frameworks, we will be given a **training** data set and a **testing** data set.\n",
    "\n",
    "Each dataset consists of a single .csv file.\n",
    "\n",
    "A CSV file is a common-separated-values format where each line in the file is a row in a table, where each column is separated by commas. In our case each line will contain 3 columns of data, an id, a quote, and the author.\n",
    "\n",
    "Example:\n",
    "\n",
    "`\"id10633\",\"It was the beating of the old man's heart.\",\"EAP\"`\n",
    "\n",
    "In the above there is an id: `\"id10633\"`, a quote: `\"It was the beating of hte old man's heart.\"`, and the author: `\"EAP\"` denoting Edgar Allen Poe.\n",
    "\n",
    "Each author will be denoted by initials: `EAP` for Edgar Allen Poe, `MWS` for Mary Wollstonecraft Shelley and `HPL` for H.P. Lovecraft.\n",
    "\n",
    "We will use the `train.csv` dataset to build up a model for the frequencies of words each of our 3 authors use. Then we will see how this simple model can identify each model when we show it the test.csv dataset.\n",
    "\n",
    "* Note: mostly we will be practicing some of our programming skills like dictionary use rather than building a quality NLP (natural language processing) model to accomplish this task. But you can always explore the actual approaches people used at the above kaggle link. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba152fde-46e2-40b1-80bd-82840c3a4440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this case study to your own drive\n",
    "\n",
    "# cell to download the data files\n",
    "\n",
    "# the following code will download the data files required for this program\n",
    "\n",
    "# TODO EDIT THIS ONCE COLAB IS CREATED\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b16950-dc80-4d51-b295-03e28ded25a0",
   "metadata": {},
   "source": [
    "## csv Module\n",
    "\n",
    "Rather than writing our own csv parsing code we will use the csv module. This module has a `DictReader` class that is capable of \n",
    "converting each row of the csv file into its own dictionary. In our case it will have `id`, `quote` and `author` keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2b87495-cf43-4f00-a27d-c0d20029b019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'id04853', 'quote': 'They still appeared in public together, and lived under the same roof.', 'author': 'MWS'}\n",
      "{'id': 'id21482', 'quote': '\"But, my dear fellow, you are joking then,\" said I, \"this is a very passable skull indeed, I may say that it is a very excellent skull, according to the vulgar notions about such specimens of physiology and your scarabæus must be the queerest scarabæus in the world if it resembles it.', 'author': 'EAP'}\n",
      "\n",
      "\n",
      "total lines read: 19479\n"
     ]
    }
   ],
   "source": [
    "#use the csv module to load the data\n",
    "import csv\n",
    "with open('train.csv') as csvfile:\n",
    "\n",
    "    #DictReader will populate a dictionary for each line in the csv\n",
    "    reader = csv.DictReader(csvfile, fieldnames=[\"id\", \"quote\", \"author\"])\n",
    "\n",
    "    #create a counter so we can just print the first two records in the file\n",
    "    cnt = 0\n",
    "    #each row is a dictionary\n",
    "    for row in reader:\n",
    "        if cnt < 2:\n",
    "            print(row)\n",
    "        cnt += 1\n",
    "    \n",
    "    print(\"\\n\\ntotal lines read:\", reader.line_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec8c8e5-a4bb-49a6-a549-5231a5032bfa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Group Task 2: Understanding the data (5 minutes)\n",
    "\n",
    "*The group paper recorder should put a paper in the middle of the work space so all can see*\n",
    "\n",
    "In your groups discuss the above code and below questions and record your answers on paper.\n",
    "\n",
    "1. **How many different authors are their in our dataset?**\n",
    "\n",
    "2. **The training dataset has 19479 records each made up of 3 fields: `id`, `quote` and `author`, which of the fields is least important to our goal?**\n",
    "\n",
    "In the above code the variable `row` is initialized as a dictionary with keys: `id`, `quote` and `author` for each record/line in the `train.csv` data file. Recall we want to know how frequently words are used by each author.\n",
    "\n",
    "3. **What do we want to do with each of these rows?**\n",
    "\n",
    "i.e., in plain english inside the `for row in reader:` loop, what do we want to have happen?\n",
    "\n",
    "Things to consider:\n",
    "\n",
    "**1. How do we access the data inside `row`?**\n",
    "\n",
    "\n",
    "**2. How do we want our data organized?**\n",
    "\n",
    "\n",
    "**3. How should we process each row?**\n",
    "\n",
    "\n",
    "**4. Do we need to do any data pre-processing?**\n",
    "\n",
    "**5. Other considerations**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edd9b63-7e06-4be5-a88f-46bf86517c79",
   "metadata": {},
   "source": [
    "# Task 3 Pseudo Code on Paper (5 minutes) \n",
    "\n",
    "Write out a high level pseudo-code solution on paper for how to process the training data. High level means avoid the details. We are looking for less than 6 high level steps for how to process our train.csv file, i.e., how to determine the frequency of word use for each author.\n",
    "\n",
    "0. initialize a dictionary for each author to track the counts of each word\n",
    "1. examine each record\n",
    "2. pre-process/clean the quote part of the record\n",
    "3. for each word in the clean quote do the following:\n",
    "\n",
    "   a. increment a count for this word in a dictionary for the record's author\n",
    "\n",
    "   b. increment a counter of the total number of words for the record's author\n",
    "\n",
    "4. Convert the word counts into frequencies by dividing by total count for each author\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b12067-b029-4cfe-affa-37f11804b5ca",
   "metadata": {},
   "source": [
    "## Pre-processing \n",
    "\n",
    "We will clean the quote by pre-processing it in the following ways:\n",
    "\n",
    "1. To avoid issues with punctuation, i.e., \"hello,\" being a different word from \"hello\" we will remove all punctuation from the quotes. \n",
    "2. To avoid issues with capitalization, i.e., \"hello\" being different from \"Hello\" we will convert all quotes to lower case\n",
    "3. To avoid issues with common words, i.e., analyzing \"and\", \"I\" or \"a\" we will remove these `STOP` words from the quote\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299cffe9-f643-4c5e-909f-f80102a709dd",
   "metadata": {},
   "source": [
    "## Task 4: Implement the below clean_quote method (5 minutes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b32190d-1eea-497e-9b1a-521e0b10c84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import string\n",
    "\n",
    "#Task 4-1: Ask generative ai to suggest some stop words in a python list (including those from 1800 and 1900s writing. \n",
    "#          Populate them into the below list\n",
    "\n",
    "stop_words = [\n",
    "    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\",\n",
    "    \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\",\n",
    "    \"could\",\n",
    "    \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\",\n",
    "    \"each\",\n",
    "    \"few\", \"for\", \"from\", \"further\",\n",
    "    \"had\", \"has\", \"have\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\",\n",
    "    \"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\",\n",
    "    \"let's\",\n",
    "    \"me\", \"more\", \"most\", \"my\", \"myself\",\n",
    "    \"nor\", \"not\",\n",
    "    \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\n",
    "    \"same\", \"she\", \"should\", \"so\", \"some\", \"such\",\n",
    "    \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\",\n",
    "    \"under\", \"until\", \"up\",\n",
    "    \"very\",\n",
    "    \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"with\", \"would\",\n",
    "    \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
    "\"thou\", \"thee\", \"thy\", \"thine\", \"ye\", \"hath\", \"doth\", \"dost\", \"art\", \"shalt\", \"wilt\", \"hast\", \"wert\",\n",
    "    \"whilst\", \"ere\", \"oft\", \"nay\", \"yea\", \"saith\", \"unto\", \"wherefore\", \"whence\", \"whither\"\n",
    "]\n",
    "\n",
    "print(string.punctuation)\n",
    "\n",
    "def clean_quote(quote :str, stop_words: list[str]) -> list[str]:\n",
    "    \"\"\" For a given quote clean the quote in 3 ways\n",
    "    1. remove punctuation\n",
    "    2. convert to lower case\n",
    "    3. remove stop words \n",
    "\n",
    "    @param: quote (string) the given quote to clean\n",
    "    @param: stop_words (list of strings) a list of all of the words that should be removed from quote\n",
    "\n",
    "    @return a list of strings representing the words in the cleaned quote\n",
    "    \"\"\"\n",
    "\n",
    "    #step 1 convert the quote to lower case\n",
    "\n",
    "\n",
    "    #step 2: remove punctuation (keep only characters that aren't punctuation)\n",
    "    #        note: that the string module has a list of common punctuation \n",
    "    #              in a variable: string.punctuation\n",
    "   \n",
    "\n",
    "    #step 3: keep only the non-stop words\n",
    "    #        note: if you split a string by \" \" (space) \n",
    "    #              you get a list of the \"words\" in that string you can iterate over\n",
    "\n",
    "    #step 4: return a list of words that have been cleaned\n",
    "\n",
    "    #clean the quote\n",
    "    # Filter out punctuation\n",
    "    clean = ''.join([char.lower() for char in quote if char not in string.punctuation])\n",
    "\n",
    "    #filter out stop words:\n",
    "    good_words = [word for word in clean.split() if word not in stop_words]\n",
    "\n",
    "    return good_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2855264a-e1b8-4c7f-a0b3-9768dbf37774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#once you have completed the above clean_quote the following code should produce no errors\n",
    "#don't forget to \"run\" the above cell before running this cell\n",
    "\n",
    "good_words = clean_quote(\"You say, \\\"Goodbye\\\" and I say, \\\"Hello, hello, HELLO\\\"!\", stop_words)\n",
    "assert \"HELLO\" not in good_words, \"capital HELLO test failed\"\n",
    "assert \"i\" not in good_words, \"stop word test failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcf2991-78c6-41ee-92f3-12091d041a0d",
   "metadata": {},
   "source": [
    "## Task 5: Implement Count Words and Convert to Frequency methods ( 5 minutes )\n",
    "\n",
    "`count_words` is only a couple of lines just be careful when dealing with words that might not yet be in the dictionary\n",
    "\n",
    "`convert_to_freqs` does not take as input the total number of words, how can we obtain this easily from input_dict?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1939f4f2-fef2-484b-bf93-875ef7a07e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(good_words :str, author_dict: dict[str, int]) -> None:\n",
    "    \"\"\" Given a list of words and a dictionary of word counts\n",
    "        increment the count for each word in the list\n",
    "\n",
    "        @param good_words a list of cleaned words\n",
    "        @param author_dict a dictionary with keys as strings (words) and \n",
    "                           value int representing the count of occurrences of that word\n",
    "        @return None nothing is returned\n",
    "    \"\"\"\n",
    "    for word in good_words:\n",
    "        author_dict[word] = author_dict.get(word, 0) + 1\n",
    "    \n",
    "    \n",
    "\n",
    "def convert_to_freqs(input_dict: dict[str, int]) -> dict[str, float]:\n",
    "    \"\"\" For a given input dictionary of words (str) and their respective counts (int)\n",
    "       return a new dictionary with the words (str) and their frequency of occurrence (float)\n",
    "    \n",
    "        @param input_dict a dictionary of words and respective counts\n",
    "        @return dictionary with the same keys as input_dict but having values as the \n",
    "                frequency of occurrence of those words\n",
    "    \"\"\"\n",
    "    ttl = sum(input_dict.values())\n",
    "    output_dict = {}\n",
    "    for word in input_dict:\n",
    "        output_dict[word] = input_dict[word]/ttl\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52255f9f-d378-465d-a078-95cecb4a2836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'say': 0.3333333333333333, 'goodbye': 0.16666666666666666, 'hello': 0.5}\n",
      "all tests complete\n"
     ]
    }
   ],
   "source": [
    "# when you complete your methods this code should run properly\n",
    "\n",
    "author_dict = {}\n",
    "good_words = [\"say\", \"goodbye\", \"say\", \"hello\", \"hello\", \"hello\"]\n",
    "count_words(good_words, author_dict)\n",
    "assert \"hello\" in author_dict and author_dict[\"hello\"] == 3, \"hello test failed expecting count=3\"\n",
    "assert \"i\" not in author_dict, \"stop word test failed, expecting \\\"i\\\" is a stop word\"\n",
    "assert len(author_dict) == len([\"say\", \"goodbye\", \"hello\"]), \"number of keys should be 3 test failed\"\n",
    "\n",
    "freq_dict = convert_to_freqs(author_dict)\n",
    "print(freq_dict)\n",
    "assert len(freq_dict) == len(author_dict), \"len should match output and input\"\n",
    "assert abs(freq_dict.get(\"goodbye\", 0) - author_dict.get(\"goodbye\", 0.1)/sum(author_dict.values())) < 0.00001, \"check freq calculation\"\n",
    "\n",
    "print(\"all tests complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cdb0bd7-d773-4524-a8d4-7695eca51139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: They still appeared in public together, and lived under the same roof.\n",
      "cleaned quote: ['still', 'appeared', 'public', 'together', 'lived', 'roof']\n",
      "original: \"But, my dear fellow, you are joking then,\" said I, \"this is a very passable skull indeed, I may say that it is a very excellent skull, according to the vulgar notions about such specimens of physiology and your scarabæus must be the queerest scarabæus in the world if it resembles it.\n",
      "cleaned quote: ['dear', 'fellow', 'joking', 'said', 'passable', 'skull', 'indeed', 'may', 'say', 'excellent', 'skull', 'according', 'vulgar', 'notions', 'specimens', 'physiology', 'scarabæus', 'must', 'queerest', 'scarabæus', 'world', 'resembles']\n",
      "\n",
      "\n",
      "total quotes read: 19479\n",
      "total EAP words: 15231\n",
      "total MWS words: 11427\n",
      "total HPL words: 14535\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Glue code \n",
    "The following code will process our training dataset by iterating over the records and calling our above methods\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#create 3 dictionaries to store the count of each word for each author\n",
    "eap = {}\n",
    "mws = {}\n",
    "hpl = {}\n",
    "\n",
    "\n",
    "with open('train.csv') as csvfile:\n",
    "\n",
    "    #DictReader will populate a dictionary for each line in the csv\n",
    "    reader = csv.DictReader(csvfile, fieldnames=[\"id\", \"quote\", \"author\"])\n",
    "\n",
    "    #create a counter so we can just print the first two records in the file\n",
    "    cnt = 0\n",
    "    for row in reader:\n",
    "\n",
    "        #task 4: clean the input and give back a list of cleaned words\n",
    "        good_words = clean_quote(row['quote'], stop_words)\n",
    "\n",
    "        #print the first 2 quotes so we can see them\n",
    "        if cnt < 2:\n",
    "            print(\"original:\", row['quote'])\n",
    "            print (\"cleaned quote:\", good_words)\n",
    "        cnt += 1\n",
    "\n",
    "        author = row['author']\n",
    "\n",
    "        #set the correct dictionary\n",
    "        author_dict = eap\n",
    "        if author == \"MWS\":\n",
    "            author_dict = mws\n",
    "\n",
    "        elif author == \"HPL\":\n",
    "            author_dict = hpl\n",
    "\n",
    "        #Task 5 count of the cleaned words in an author_dictionary\n",
    "        count_words(good_words, author_dict)\n",
    "        \n",
    "    print(\"\\n\\ntotal quotes read:\", reader.line_num)\n",
    "\n",
    "\n",
    "#task 5: convert the dictionary of counts into a dictionary of frequencies\n",
    "eap_f = convert_to_freqs(eap)\n",
    "mws_f = convert_to_freqs(mws)\n",
    "hpl_f = convert_to_freqs(hpl)\n",
    "\n",
    "\n",
    "print(\"total EAP words:\", len(eap))\n",
    "print(\"total MWS words:\",len(mws))\n",
    "print(\"total HPL words:\",len(hpl))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ee9609-14e4-4453-a0f1-0b5eff449355",
   "metadata": {},
   "source": [
    "## Task 6 Discussion and Pseudo-code (5 minutes)\n",
    "\n",
    "We have processed the training dataset and have 3 dictionaries giving the frequency of use of an authors words. Our algorithm is to compare an unseen quote to our model (3 dictionaries of author word frequencies) and then predict the author of the unseen quote by computing the product of frequencies for the words in the unseen quote. \n",
    "\n",
    "Our product of frequencies may end up multiplying many small numbers together, which can be problematic, to avoid this we will only multiply together the *X* words having the largest frequency for that author.\n",
    "\n",
    "Discuss how to implement this with your group and form pseudo code on paper for your solution\n",
    "\n",
    "Discussion points:\n",
    "\n",
    "1. Can we reuse any of our work from prior tasks?\n",
    "\n",
    "2. What happens if the frequency for a word is zero? What happens if a word shows up twice in the same quote?\n",
    "\n",
    "3. How can we limited our product calculation to include only the *k* (depth) largest values?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc495ed-58d8-49b2-899d-4ef86575b5e6",
   "metadata": {},
   "source": [
    "## Task 7 Implement the product_freq method (7 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66300532-598a-46d2-8247-7b748bf58f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to frequencies\n",
    "\n",
    "\n",
    "\n",
    "# All groups for a given cleaned quote and author \n",
    "# determine the product of frequencies \n",
    "# for the largest depth number of terms in the quote\n",
    "def product_freq(clean_quote:list[str], author_freq_dict:str, depth:int) -> float:\n",
    "    \"\"\"\n",
    "    For a given cleaned quote (list of strings) and an author_freq_dict of words and their \n",
    "    frequency of use, compute the product of frequencies for the words having the depth (int)\n",
    "    largest frequencies\n",
    "\n",
    "    In the case that a word in clean_quote is not present in the author_freq_dict then small_number should be its frequency.\n",
    "\n",
    "    @param clean_quote: a list of words \n",
    "    @param author_freq_dict: A dictionary of words and their associated frequency of use for a particular author\n",
    "    @param depth: The number of words to use in the product calculation (only the depth-most words), if depth is larger than \n",
    "                  the length of the quote then only len(clean_quote) words are used in the product. Depth is always larger than 0\n",
    "\n",
    "                  \n",
    "    @return a floating point number giving the product of word frequencies for the maximum depth largest frequencies.\n",
    "    \n",
    "    \"\"\"\n",
    "    small_number = 0.0000001\n",
    "    \n",
    "    freqs = []\n",
    "    for word in clean_quote:\n",
    "        freqs.append(author_freq_dict.get(word,0))\n",
    "    freqs.sort(reverse=True)\n",
    "\n",
    "    cnt = 0\n",
    "    prod = 1\n",
    "    while cnt < depth and cnt < len(freqs):\n",
    "        if freqs[cnt] == 0:\n",
    "            prod *= small_number\n",
    "        else:\n",
    "            prod *= freqs[cnt]\n",
    "        cnt += 1\n",
    "\n",
    "    #but don't return 1 as perfect match\n",
    "    if prod == 1:\n",
    "        return 0\n",
    "    \n",
    "    return prod\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df897f11-6f0c-4cc7-a219-b413df514f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "above test results\n"
     ]
    }
   ],
   "source": [
    "#once the above is complete the following tests should pass\n",
    "clean_q = [\"worst\", \"episode\", \"ever\"]\n",
    "author_freq_dict = { \"lisa\": 0.1, \"needs\": 0.2, \"braces\":0.2, \"episode\": 0.35, \"ever\":0.15}\n",
    "\n",
    "#be careful this needs to match above small_number\n",
    "small_number = 0.0000001\n",
    "threshold = 0.000001\n",
    "\n",
    "#depth longer than quote\n",
    "prod = product_freq(clean_q, author_freq_dict, 4)\n",
    "assert abs(prod - .15 * 0.35 * small_number * small_number) < threshold, f\"product frequency expecting {.15*.35} got {prod}\"\n",
    "\n",
    "#depth longer than author_freq_dict\n",
    "prod = product_freq(clean_q, author_freq_dict, 3)\n",
    "assert abs(prod - .15 * 0.35 * small_number) < threshold, f\"product frequency expecting {.15*.35} got {prod}\"\n",
    "\n",
    "#depth equals clean length\n",
    "prod = product_freq(clean_q, author_freq_dict, 2)\n",
    "assert abs(prod - .15 * 0.35) < threshold, f\"product frequency expecting {.15*.35} got {prod}\"\n",
    "\n",
    "#depth shorter than quote\n",
    "prod = product_freq(clean_q, author_freq_dict, 1)\n",
    "assert abs(prod - 0.35) < threshold, f\"product frequency expecting .35 got {prod}\"\n",
    "\n",
    "print(\"above test results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "844cd6ce-1bc5-4fa0-9fad-a6f032c26b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EAP['process', 'however', 'afforded', 'no', 'means', 'ascertaining', 'dimensions', 'dungeon', 'might', 'make', 'circuit', 'return', 'point', 'set', 'without', 'aware', 'fact', 'perfectly', 'uniform', 'seemed', 'wall']  (1.5586488775254633e-28, 'EAP')\n",
      "HPL['never', 'occurred', 'fumbling', 'might', 'mere', 'mistake']  (6.151356442660928e-22, 'HPL')\n",
      "EAP['left', 'hand', 'gold', 'snuff', 'box', 'capered', 'hill', 'cutting', 'manner', 'fantastic', 'steps', 'took', 'snuff', 'incessantly', 'air', 'greatest', 'possible', 'self', 'satisfaction']  (4.5289060283833936e-32, 'EAP')\n",
      "MWS['lovely', 'spring', 'looked', 'windsor', 'terrace', 'sixteen', 'fertile', 'counties', 'spread', 'beneath', 'speckled', 'happy', 'cottages', 'wealthier', 'towns', 'looked', 'former', 'years', 'heart', 'cheering', 'fair']  (2.4034090269029607e-31, 'MWS')\n",
      "HPL['finding', 'nothing', 'else', 'even', 'gold', 'superintendent', 'abandoned', 'attempts', 'perplexed', 'look', 'occasionally', 'steals', 'countenance', 'sits', 'thinking', 'desk']  (1.0472996962439154e-34, 'EAP')\n",
      "MWS['youth', 'passed', 'solitude', 'best', 'years', 'spent', 'gentle', 'feminine', 'fosterage', 'refined', 'groundwork', 'character', 'cannot', 'overcome', 'intense', 'distaste', 'usual', 'brutality', 'exercised', 'board', 'ship', 'never', 'believed', 'necessary', 'heard', 'mariner', 'equally', 'noted', 'kindliness', 'heart', 'respect', 'obedience', 'paid', 'crew', 'felt', 'peculiarly', 'fortunate', 'able', 'secure', 'services']  (9.003605479304908e-30, 'MWS')\n",
      "EAP['astronomer', 'perhaps', 'point', 'took', 'refuge', 'suggestion', 'non', 'luminosity', 'analogy', 'suddenly', 'let', 'fall']  (5.287355948437212e-36, 'EAP')\n",
      "EAP['surcingle', 'hung', 'ribands', 'body']  (1.611439840240391e-18, 'EAP')\n",
      "EAP['knew', 'say', 'stereotomy', 'without', 'brought', 'think', 'atomies', 'thus', 'theories', 'epicurus', 'since', 'discussed', 'subject', 'long', 'ago', 'mentioned', 'singularly', 'yet', 'little', 'notice', 'vague', 'guesses', 'noble', 'greek', 'met', 'confirmation', 'late', 'nebular', 'cosmogony', 'felt', 'avoid', 'casting', 'eyes', 'upward', 'great', 'nebula', 'orion', 'certainly', 'expected']  (5.243757194166925e-28, 'EAP')\n",
      "MWS['confess', 'neither', 'structure', 'languages', 'code', 'governments', 'politics', 'various', 'states', 'possessed', 'attractions']  (7.646764076356664e-42, 'MWS')\n",
      "MWS['shall', 'find', 'can', 'feel', 'injuries', 'shall', 'learn', 'dread', 'revenge', 'days', 'arrived']  (1.132797503908312e-31, 'MWS')\n",
      "EAP['barricaded', 'present', 'secure']  (2.3867002911633073e-13, 'HPL')\n",
      "HPL['herbert', 'west', 'needed', 'fresh', 'bodies', 'life', 'work', 'reanimation', 'dead']  (3.977827610689329e-31, 'HPL')\n",
      "HPL['farm', 'like', 'grounds', 'extended', 'back', 'deeply', 'hill', 'almost', 'wheaton', 'street']  (1.131952077626385e-38, 'HPL')\n",
      "EAP['glance', 'will', 'show', 'fallacy', 'idea']  (2.7421181283914995e-18, 'EAP')\n",
      "MWS['escaped', 'must', 'commence', 'destructive', 'almost', 'endless', 'journey', 'across', 'mountainous', 'ices', 'ocean', 'amidst', 'cold', 'inhabitants', 'long', 'endure', 'native', 'genial', 'sunny', 'climate', 'hope', 'survive']  (9.738353631782609e-32, 'MWS')\n",
      "EAP['speeches', 'gave', 'course', 'interpretation', 'fancying', 'no', 'doubt', 'events', 'come', 'possession', 'vast', 'quantities', 'ready', 'money', 'provided', 'paid', 'owed', 'trifle', 'consideration', 'services', 'dare', 'say', 'cared', 'little', 'became', 'either', 'soul', 'carcass']  (2.5758178074355985e-29, 'EAP')\n",
      "MWS['native', 'sprightliness', 'needed', 'no', 'undue', 'excitement', 'placid', 'heart', 'reposed', 'contented', 'love', 'well', 'children', 'beauty', 'surrounding', 'nature']  (1.4125537093414653e-30, 'MWS')\n",
      "EAP['even', 'went', 'far', 'speak', 'slightly', 'hectic', 'cough', 'one', 'time', 'troubled', 'chronic', 'rheumatism', 'twinge', 'hereditary', 'gout', 'conclusion', 'disagreeable', 'inconvenient', 'hitherto', 'carefully', 'concealed', 'weakness', 'eyes']  (1.1886174618529949e-30, 'EAP')\n",
      "HPL['facial', 'aspect', 'remarkable', 'maturity', 'though', 'shared', 'mothers', 'grandfathers', 'chinlessness', 'firm', 'precociously', 'shaped', 'nose', 'united', 'expression', 'large', 'dark', 'almost', 'latin', 'eyes', 'give', 'air', 'quasi', 'adulthood', 'well', 'nigh', 'preternatural', 'intelligence']  (1.7161279086275788e-31, 'HPL')\n",
      "EAP['now', 'net', 'work', 'permanently', 'fastened', 'hoop', 'attached', 'series', 'running', 'loops', 'nooses']  (1.2094601573612509e-39, 'EAP')\n",
      "HPL['sounds', 'hideous', 'held', 'vibrations', 'suggesting', 'nothing', 'globe', 'earth', 'certain', 'intervals', 'assumed', 'symphonic', 'quality', 'hardly', 'conceive', 'produced', 'one', 'player']  (2.3051918011307008e-32, 'HPL')\n",
      "EAP['every', 'hand', 'wilderness', 'balconies', 'verandas', 'minarets', 'shrines', 'fantastically', 'carved', 'oriels']  (7.795956901137975e-48, 'HPL')\n",
      "EAP['deep', 'spirit', 'wonder', 'perplexity', 'wont', 'regard', 'remote', 'pew', 'gallery', 'step', 'solemn', 'slow', 'ascended', 'pulpit', 'reverend', 'man', 'countenance', 'demurely', 'benign', 'robes', 'glossy', 'clerically', 'flowing', 'wig', 'minutely', 'powdered', 'rigid', 'vast', 'late', 'sour', 'visage', 'snuffy', 'habiliments', 'administered', 'ferule', 'hand', 'draconian', 'laws', 'academy']  (2.0816250545999835e-32, 'MWS')\n",
      "EAP['bizarre', 'attempts', 'explanation', 'followed', 'others', 'equally', 'bizarre']  (5.363155695064451e-27, 'HPL')\n",
      "EAP['many', 'prodigies', 'signs', 'taken', 'place', 'far', 'wide', 'sea', 'land', 'black', 'wings', 'pestilence', 'spread', 'abroad']  (6.43910142003844e-32, 'HPL')\n",
      "EAP['yet', 'can', 'fairly', 'said', 'known', 'pure', 'gold', 'can', 'made', 'will', 'readily', 'lead', 'connection', 'certain', 'substances', 'kind', 'proportions', 'unknown', 'speculation', 'course', 'busy', 'immediate', 'ultimate', 'results', 'discovery', 'discovery', 'thinking', 'persons', 'will', 'hesitate', 'referring', 'increased', 'interest', 'matter', 'gold', 'generally', 'late', 'developments', 'california', 'reflection', 'brings', 'us', 'inevitably', 'another', 'exceeding', 'inopportuneness', 'von', 'kempelens', 'analysis']  (8.272049621586236e-27, 'MWS')\n",
      "EAP['seemed', 'upon', 'verge', 'comprehension', 'without', 'power', 'comprehend', 'men', 'times', 'find', 'upon', 'brink', 'remembrance', 'without', 'able', 'end', 'remember']  (7.886464257526831e-29, 'EAP')\n",
      "HPL['compasses', 'depth', 'gauges', 'delicate', 'instruments', 'ruined', 'henceforth', 'reckoning', 'guesswork', 'based', 'watches', 'calendar', 'apparent', 'drift', 'judged', 'objects', 'might', 'spy', 'portholes', 'conning', 'tower']  (2.009129404993713e-38, 'HPL')\n",
      "HPL['young', 'warriors', 'took', 'back', 'sarnath', 'symbol', 'conquest', 'old', 'gods', 'beings', 'ib', 'sign', 'leadership', 'mnar']  (4.817231555925749e-34, 'HPL')\n",
      "EAP['meantime', 'whole', 'paradise', 'arnheim', 'bursts', 'upon', 'view']  (1.678867663310056e-28, 'EAP')\n",
      "MWS['rich', 'young', 'guardian', 'appointed', 'act', 'one', 'great', 'society', 'must', 'keep', 'secret', 'really', 'cut', 'ever']  (2.7081035231680555e-31, 'HPL')\n",
      "MWS['make', 'little', 'dim', 'light', 'seemed', 'contain', 'prophecies', 'detailed', 'relations', 'events', 'lately', 'passed', 'names', 'now', 'well', 'known', 'modern', 'date', 'often', 'exclamations', 'exultation', 'woe', 'victory', 'defeat', 'traced', 'thin', 'scant', 'pages']  (4.341396215457026e-30, 'EAP')\n",
      "HPL['even', 'now', 'talked', 'tombs']  (2.9469271037944195e-13, 'HPL')\n",
      "HPL['sheehan', 'especially', 'ply', 'inquiries', 'yet', 'without', 'eliciting', 'information', 'value', 'concerning', 'old', 'bugs']  (5.542626189985637e-36, 'HPL')\n",
      "HPL['cried', 'aloud', 'little', 'later', 'gave', 'gasp', 'terrible', 'cry']  (2.789586617079334e-29, 'HPL')\n",
      "HPL['old', 'tracks', 'crossed', 'river', 'street', 'grade', 'veered', 'region', 'increasingly', 'rural', 'less', 'less', 'innsmouths', 'abhorrent', 'fishy', 'odour']  (8.961596303083974e-34, 'HPL')\n",
      "MWS['soul', 'overflowed', 'ardent', 'affections', 'friendship', 'devoted', 'wondrous', 'nature', 'world', 'minded', 'teach', 'us', 'look', 'imagination']  (4.889606185880215e-32, 'MWS')\n",
      "HPL['first', 'start', 'replaced', 'tissue', 'wrapping', 'around', 'portrait', 'shield', 'sordidness', 'place']  (1.1369593849142981e-44, 'MWS')\n",
      "EAP['present', 'peculiar', 'condition', 'affairs', 'court', 'especially', 'intrigues', 'd', 'known', 'involved', 'render', 'instant', 'availability', 'document', 'susceptibility', 'produced', 'moments', 'notice', 'point', 'nearly', 'equal', 'importance', 'possession']  (6.962286330831258e-33, 'EAP')\n",
      "HPL['wilburs', 'growth', 'indeed', 'phenomenal', 'within', 'three', 'months', 'birth', 'attained', 'size', 'muscular', 'power', 'usually', 'found', 'infants', 'full', 'year', 'age']  (8.951135730816219e-32, 'EAP')\n",
      "HPL['pausing', 'succeeded', 'difficulty', 'raising', 'whereupon', 'revealed', 'black', 'aperture', 'exhaling', 'noxious', 'fumes', 'caused', 'torch', 'sputter', 'disclosing', 'unsteady', 'glare', 'top', 'flight', 'stone', 'steps']  (3.7994404299075716e-36, 'HPL')\n",
      "HPL['mud', 'water', 'sky', 'dark', 'rain', 'wipin', 'aout', 'tracks', 'abaout', 'fast', 'beginnin', 'glen', 'maouth', 'whar', 'trees', 'moved', 'still', 'o', 'awful', 'prints', 'big', 'barls', 'like', 'seen', 'monday']  (9.167709882915747e-31, 'HPL')\n",
      "MWS['visits', 'merrival', 'windsor', 'frequent', 'suddenly', 'ceased']  (2.2175108825868883e-22, 'MWS')\n",
      "EAP['supposed', 'however', 'great', 'underduk', 'suffered', 'impertinence', 'part', 'little', 'old', 'man', 'pass', 'impunity']  (8.653633725048373e-32, 'EAP')\n",
      "EAP['need', 'tell', 'sceptical', 'hitherto', 'topic', 'souls', 'immortality']  (3.55470182881121e-28, 'EAP')\n",
      "MWS['often', 'compared', 'finding', 'chief', 'superiority', 'consisted', 'power', 'soon', 'persuaded', 'power', 'inferior', 'chiefest', 'potentates', 'earth']  (7.209594591421733e-35, 'MWS')\n",
      "HPL['childrens', 'children', 'newcomers', 'children', 'grew']  (7.32484222357712e-22, 'MWS')\n",
      "HPL['dr', 'johnson', 'beheld', 'full', 'pursy', 'man', 'ill', 'drest', 'slovenly', 'aspect']  (7.8542423098243e-43, 'HPL')\n",
      "EAP['presently', 'murmur', 'water', 'fell', 'gently', 'upon', 'ear', 'moments', 'afterward', 'turned', 'road', 'somewhat', 'abruptly', 'hitherto', 'became', 'aware', 'building', 'kind', 'lay', 'foot', 'gentle', 'declivity', 'just']  (1.0441081792434522e-30, 'EAP')\n",
      "EAP['ellison', 'remarkable', 'continuous', 'profusion', 'good', 'gifts', 'lavished', 'upon', 'fortune']  (7.5087538147639526e-34, 'EAP')\n",
      "EAP['still', 'continued', 'plane', 'elipse', 'made', 'little', 'progress', 'eastward']  (1.8042191683651095e-28, 'EAP')\n",
      "MWS['useless', 'provide', 'many', 'things', 'find', 'abundant', 'provision', 'every', 'town']  (1.4497949999927965e-32, 'MWS')\n",
      "MWS['fly', 'quickly', 'snow', 'sledges', 'motion', 'pleasant', 'opinion', 'far', 'agreeable', 'english', 'stagecoach']  (2.2166832007925692e-40, 'MWS')\n",
      "MWS['pointed', 'spot', 'disappeared', 'followed', 'track', 'boats', 'nets', 'cast', 'vain']  (4.184069788659829e-36, 'HPL')\n",
      "HPL['indistinct', 'recollections', 'great', 'storm', 'time', 'reached', 'boat', 'rate', 'know', 'heard', 'peals', 'thunder', 'tones', 'nature', 'utters', 'wildest', 'moods']  (1.2923223238994905e-32, 'HPL')\n",
      "HPL['seemed', 'no', 'one', 'courtyard', 'hoped', 'chance', 'get', 'away', 'spreading', 'general', 'alarm']  (1.5179293731160475e-32, 'HPL')\n",
      "EAP['mein', 'gott', 'take', 'vor', 'shicken', 'no', 'oh', 'no', 'replied', 'much', 'alarmed', 'no', 'chicken', 'certainly']  (5.763358966961016e-31, 'EAP')\n",
      "MWS['perpetual', 'fear', 'jaundiced', 'complexion', 'shrivelled', 'whole', 'person']  (4.712643407764108e-30, 'MWS')\n",
      "MWS['sun', 'set', 'atmosphere', 'grew', 'dim', 'evening', 'star', 'no', 'longer', 'shone', 'companionless']  (5.046403055370903e-34, 'MWS')\n",
      "MWS['rain', 'ceased', 'clouds', 'sunk', 'behind', 'horizon', 'now', 'evening', 'sun', 'descended', 'swiftly', 'western', 'sky']  (4.368444992586143e-34, 'MWS')\n",
      "HPL['nothing', 'however', 'occurred', 'except', 'hill', 'noises', 'day', 'came', 'many', 'hoped', 'new', 'horror', 'gone', 'swiftly', 'come']  (9.246749005608839e-30, 'HPL')\n",
      "EAP['state', 'rooms', 'sufficiently', 'roomy', 'two', 'berths', 'one']  (6.684887237435267e-28, 'EAP')\n",
      "HPL['getting', 'dark', 'ancient', 'roofs', 'chimney', 'pots', 'outside', 'looked', 'queer', 'bulls', 'eye', 'window', 'panes']  (4.580825901613919e-33, 'HPL')\n",
      "MWS['marched', 'various', 'parts', 'southern', 'counties', 'quartered', 'deserted', 'villages', 'part', 'sent', 'back', 'island', 'season', 'winter', 'far', 'revived', 'energy', 'passes', 'country', 'defended', 'increase', 'numbers', 'prohibited']  (1.4440639621904878e-33, 'MWS')\n",
      "EAP['suffered', 'no', 'little', 'cold', 'dampness', 'atmosphere', 'unpleasant', 'ample', 'space', 'car', 'enabled', 'us', 'lie', 'means', 'cloaks', 'blankets', 'sufficiently', 'well']  (1.3734398129784042e-30, 'EAP')\n",
      "MWS['dared', 'conquered', 'till', 'now', 'sold', 'death', 'sole', 'condition', 'shouldst', 'follow', 'fire', 'war', 'plague', 'unite', 'destruction', 'o', 'raymond', 'no', 'safety', 'heavy', 'heart', 'listened', 'changes', 'delirium', 'made', 'bed', 'cloaks', 'violence', 'decreased', 'clammy', 'dew', 'stood', 'brow', 'paleness', 'death', 'succeeded', 'crimson', 'fever', 'placed', 'cloaks']  (1.1657446079569851e-27, 'MWS')\n",
      "HPL['seemed', 'interior', 'house', 'old', 'house', 'apparently', 'details', 'inhabitants', 'constantly', 'changing', 'never', 'certain', 'faces', 'furniture', 'even', 'room', 'since', 'doors', 'windows', 'seemed', 'just', 'great', 'state', 'flux', 'presumably', 'mobile', 'objects']  (1.0063468459644881e-26, 'HPL')\n",
      "EAP['gentleman', 'clothed', 'head', 'foot', 'richly', 'embroidered', 'black', 'silk', 'velvet', 'pall', 'wrapped', 'negligently', 'around', 'form', 'fashion', 'spanish', 'cloak']  (3.266444493240275e-35, 'EAP')\n",
      "EAP['pigeons', 'appeared', 'distressed', 'extreme', 'struggled', 'escape', 'cat', 'mewed', 'piteously', 'tongue', 'hanging', 'mouth', 'staggered', 'fro', 'car', 'influence', 'poison']  (1.1947649637722852e-35, 'EAP')\n",
      "HPL['window', 'panes', 'gone']  (6.47818650458612e-11, 'HPL')\n",
      "EAP['panted', 'gasped', 'breath', 'no', 'doubt', 'design', 'tormentors', 'oh', 'unrelenting', 'oh', 'demoniac', 'men', 'shrank', 'glowing', 'metal', 'centre', 'cell']  (7.55302869803805e-34, 'EAP')\n",
      "EAP['great', 'difficulty', 'gained', 'feet', 'looking', 'dizzily', 'around', 'first', 'struck', 'idea', 'among', 'breakers', 'terrific', 'beyond', 'wildest', 'imagination', 'whirlpool', 'mountainous', 'foaming', 'ocean', 'within', 'engulfed']  (4.364568910261885e-30, 'EAP')\n",
      "MWS['next', 'morning', 'delivered', 'letters', 'introduction', 'paid', 'visit', 'principal', 'professors']  (3.0802444593272975e-36, 'MWS')\n",
      "MWS['raymond', 'make', 'end', 'without', 'drawing', 'vivid', 'glowing', 'colours', 'splendour', 'kingdom', 'opposition', 'commercial', 'spirit', 'republicanism']  (1.818364578335445e-34, 'MWS')\n",
      "EAP['celebrated', 'magazine', 'can', 'sustain', 'evidently', 'tremendous', 'expenses', 'can', 'understand']  (8.743213778728023e-35, 'EAP')\n",
      "HPL['week', 'two', 'visibly', 'faded', 'course', 'months', 'hardly', 'discernible', 'naked', 'eye']  (1.1936116660500007e-37, 'EAP')\n",
      "MWS['stranger', 'learned', 'twenty', 'words', 'first', 'lesson', 'indeed', 'understood', 'profited', 'others']  (2.4895186414559913e-36, 'MWS')\n",
      "MWS['people', 'looked', 'coming', 'struggle', 'great', 'degree', 'decisive', 'case', 'victory', 'next', 'step', 'siege', 'constantinople', 'greeks']  (8.064447681534807e-35, 'EAP')\n",
      "EAP['cavity', 'detected', 'sounding']  (1.9833105726035586e-14, 'EAP')\n",
      "MWS['lieutenant', 'instance', 'man', 'wonderful', 'courage', 'enterprise', 'madly', 'desirous', 'glory', 'rather', 'word', 'phrase', 'characteristically', 'advancement', 'profession']  (2.2460830657380623e-36, 'EAP')\n",
      "HPL['within', 'twenty', 'four', 'hours', 'machine', 'near', 'table', 'will', 'generate', 'waves', 'acting', 'unrecognised', 'sense', 'organs', 'exist', 'us', 'atrophied', 'rudimentary', 'vestiges']  (6.792160749351809e-31, 'EAP')\n",
      "HPL['everybody', 'got', 'aout', 'o', 'idee', 'o', 'dyin', 'excep', 'canoe', 'wars', 'islanders', 'sacrifices', 'sea', 'gods', 'daown', 'snake', 'bite', 'plague', 'sharp', 'gallopin', 'ailments', 'somethin', 'afore', 'cud', 'take', 'water', 'simply', 'looked', 'forrad', 'kind', 'o', 'change', 'want', 'bit', 'horrible', 'arter']  (4.460567328471499e-31, 'HPL')\n",
      "EAP['lips', 'usual', 'marble', 'pallor']  (1.4083248315567676e-15, 'EAP')\n",
      "EAP['absolutely', 'needless', 'replied', 'g']  (2.8230682102246304e-15, 'EAP')\n",
      "HPL['ahead', 'lay', 'sparse', 'grass', 'scrub', 'blueberry', 'bushes', 'beyond', 'naked', 'rock', 'crag', 'thin', 'peak', 'dreaded', 'grey', 'cottage']  (2.80659235419564e-35, 'HPL')\n",
      "MWS['near', 'loved', 'feel', 'limit', 'desires']  (3.366903751787766e-20, 'EAP')\n",
      "MWS['sky', 'serene', 'unable', 'rest', 'resolved', 'visit', 'spot', 'poor', 'william', 'murdered']  (6.419577270126104e-35, 'MWS')\n",
      "EAP['say', 'thing', 'observed', 'tell', 'latin', 'rem']  (1.0475762611352768e-23, 'EAP')\n",
      "MWS['said', 'threw', 'greatly', 'shade', 'cornelius', 'agrippa', 'albertus', 'magnus', 'paracelsus', 'lords', 'imagination', 'fatality', 'overthrow', 'men', 'disinclined', 'pursue', 'accustomed', 'studies']  (4.2500162196018387e-36, 'MWS')\n",
      "MWS['ex', 'queen', 'gives', 'idris', 'adrian', 'totally', 'unfitted', 'succeed', 'earldom', 'earldom', 'hands', 'becomes', 'kingdom']  (7.146593602280335e-38, 'MWS')\n",
      "EAP['pupils', 'upon', 'accession', 'diminution', 'light', 'underwent', 'contraction', 'dilation', 'just', 'observed', 'feline', 'tribe']  (2.512923116715527e-40, 'EAP')\n",
      "EAP['keep', 'largest', 'branch', 'one', 'side', 'said', 'legrand']  (4.5948278965032484e-24, 'EAP')\n",
      "EAP['quivering', 'awhile', 'among', 'draperies', 'room', 'length', 'rested', 'full', 'view', 'upon', 'surface', 'door', 'brass']  (1.3038213720548278e-31, 'EAP')\n",
      "MWS['maternal', 'affection', 'rendered', 'idris', 'selfish', 'beginning', 'calamity', 'thoughtless', 'enthusiasm', 'devoted', 'care', 'sick', 'helpless']  (1.1361714431475091e-35, 'MWS')\n",
      "MWS['came', 'like', 'protecting', 'spirit', 'poor', 'girl', 'committed', 'care', 'interment', 'friend', 'conducted', 'geneva', 'placed', 'protection', 'relation']  (6.399395018086178e-32, 'MWS')\n",
      "MWS['expected', 'extravagant', 'proposition', 'remained', 'silent', 'awhile', 'collecting', 'thoughts', 'might', 'better', 'combat', 'fanciful', 'scheme']  (1.1161071708461489e-35, 'MWS')\n",
      "EAP['matters', 'now', 'assumed', 'really', 'serious', 'aspect', 'resolved', 'call', 'upon', 'particular', 'friend', 'mr', 'theodore', 'sinivate', 'knew', 'least', 'get', 'something', 'like', 'definite', 'information']  (1.9871472602455887e-29, 'EAP')\n",
      "EAP['system', 'disadvantages', 'even', 'dangers']  (5.932089640588705e-18, 'MWS')\n",
      "HPL['everyone', 'seemed', 'inclined', 'silent', 'now', 'though', 'holding', 'secret', 'fear']  (6.88083022065388e-30, 'HPL')\n",
      "81\n",
      "100\n",
      "accuracy: 0.81\n"
     ]
    }
   ],
   "source": [
    "# Do the testing\n",
    "\n",
    "with open('test.csv') as csvfile:\n",
    "\n",
    "    #DictReader will populate a dictionary for each line in the csv\n",
    "    reader = csv.DictReader(csvfile, fieldnames=[\"id\", \"quote\", \"author\"])\n",
    "\n",
    "    #create a counter so we can just print the first two records in the file\n",
    "    cnt = 0\n",
    "    correct_cnt = 0\n",
    "    depth = 10\n",
    "    for row in reader:\n",
    "\n",
    "        cnt += 1\n",
    "        author = row['author']\n",
    "        good_words = clean_quote(row['quote'], stop_words)        \n",
    "        \n",
    "        prod_eap = (product_freq(good_words, eap_f, depth), \"EAP\")\n",
    "        prod_mws = (product_freq(good_words, mws_f, depth), \"MWS\")\n",
    "        prod_hpl = (product_freq(good_words, hpl_f, depth), \"HPL\")\n",
    "\n",
    "        winner = max(prod_eap, prod_mws, prod_hpl)\n",
    "        print(f\"{author}{good_words}  {winner}\")\n",
    "\n",
    "        if winner[1] == row['author']:\n",
    "            correct_cnt += 1\n",
    "            \n",
    "    print(correct_cnt)\n",
    "    print(cnt)\n",
    "    print(\"accuracy:\", correct_cnt/cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec6dbb7-a0fb-4e4b-add0-53279c5b05af",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "What happens in the above if depth is changed?\n",
    "\n",
    "depth = 1, 10, 25, or 100, for example\n",
    "\n",
    "What is the role of small_number?\n",
    "\n",
    "Why not just multiply by zero when we encounter words that are not in our training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92920cc6-fe8b-43bb-b48b-6398ef6e35a1",
   "metadata": {},
   "source": [
    "# Class Checkin  \n",
    "\n",
    "What accuracy level did everyone achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59d8672-e5b8-4bdd-8389-b490fb832851",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "If you found this interesting, natural language processing is an area of computer science / data science. Our approach is quite simple you might consider using cosine-similarity for comparing the \"closeness\" of things or TF-IDF (term frequency inverse document frequency) which is a way to represent word distributions.\n",
    "\n",
    "You might also consider that accuracy isn't always the best way to compare models, in some instancese it might be bad news to classify something as EAP when it wasn't but not too bad if we missed classifying something as EAP. You might read further about model validation metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ad397f-5d27-42c8-8a74-f2adab9b287c",
   "metadata": {},
   "source": [
    "# share\n",
    "\n",
    "Share your group's version of the case study with your teammates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea13046b-a053-43a7-9f2b-72c259e95457",
   "metadata": {},
   "source": [
    "# on your own \n",
    "\n",
    "Can you modify the above code to determine accuracy for each individual author? \n",
    "\n",
    "Which author did we have the highest accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9884a95-b166-4d4c-8960-6b0741913f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
